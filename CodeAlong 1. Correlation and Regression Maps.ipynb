{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9687c898-2f0a-4842-952a-baf12c7b83b4",
   "metadata": {},
   "source": [
    "# Correlation and Regression Maps: CMIP6\n",
    "### Authors\n",
    "\n",
    "Samantha Stevenson sstevenson@ucsb.edu\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[Goals](#purpose)\n",
    "\n",
    "[Import Packages](#path)\n",
    "\n",
    "[Background: ENSO, Correlation and Regression Analysis](#info)\n",
    "\n",
    "[Load and Query the CMIP6 Catalog](#load)\n",
    "\n",
    "[Read in Data](#xarray)\n",
    "\n",
    "[Calculate Regionally Averaged Time Series](#time_series)\n",
    "\n",
    "[Calculate Correlation or Regression Coefficients](#corr)\n",
    "\n",
    "[Make Maps](#maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b4dc47-5284-4465-b599-cef6bab9280c",
   "metadata": {},
   "source": [
    "<a id='purpose'></a> \n",
    "## **Goals**\n",
    "\n",
    "In this tutorial, we will work with the CMIP6 catalog hosted on Amazon Web Services. The goal this time is to use correlation and/or regression analysis to identify the places where temperature is most highly correlated with temperature in a given location (in our case: the \"NINO3.4\" region in the central equatorial Pacific Ocean). This will be done by relating _gridpoint_ information from all locations to _time series_ information for a _regional average_ of sea surface temperature.\n",
    "\n",
    "Skills provided in this tutorial:\n",
    "- Mapping of geospatial data;\n",
    "- Regional averaging;\n",
    "- Working with multiple climate variables;\n",
    "- Linear regression analysis;\n",
    "- Correlation analysis (Pearson's R)\n",
    "- Use of Python's `apply_ufunc` to apply calculations across multi-dimensional datasets\n",
    "\n",
    "### **This is the Code-Along version of tutorial 1!!**\n",
    "\n",
    "This notebook contains only a selected subset of the code for tutorial 1, so that it can be completed in a \"code along\" format in a classroom context. If you would like the fully completed version, please see \"1. Correlation and Regression Maps.ipynb\" in this repo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907ddd9-f286-45b1-903c-8d36ba757718",
   "metadata": {},
   "source": [
    "<a id='path'></a> \n",
    "## **Import Packages**\n",
    "\n",
    "As always, we begin by importing the necessary packages for our analysis. This tutorial assumes that you have all the packages needed for the [Plotting Regional Time Series](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/2.%20Plotting%20Regional%20Time%20Series%20Using%20Shapefiles.ipynb) tutorial in the EnsembleAnalysis repo installed; if you need more details on these packages, please see that tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4a239-851e-4c04-b70e-99346a6f669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import intake\n",
    "import s3fs\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970b44b-5e4a-4356-a7bf-798454a00f4e",
   "metadata": {},
   "source": [
    "<a id='info'></a> \n",
    "## **Background**\n",
    "\n",
    "Let's start with a refresher (or introduction) to the basics of correlation and regression analysis, and a common way that this is used in climate research. I'm including both correlation and regression in this tutorial for two reasons:\n",
    "1) They are both extremely commonly used across climate and environmental science; and\n",
    "2) From a coding perspective, their implementation is ALMOST identical (we'll see more on that below).\n",
    "\n",
    "### **Correlation: Pearson's R**\n",
    "\n",
    "The _correlation coefficient_, sometimes called Pearson's R, is a number that quantifies the relationship between two variables. Its value can range from -1, where the quantities are perfectly but negatively related, to +1, where the relationship is perfect and positive. In Python, there are various packages which can compute this: here we'll use the `pearsonr` function that's part of the `scipy.stats` package. That's the same package we were using in previous tutorials to calculate kernel density estimates!\n",
    "\n",
    "### **Linear Regression**\n",
    "\n",
    "The _regression coefficient_ is different from the correlation: it represents the _slope_ of the linear fit between two variables. There are many resources available online describing the basic principles of linear regression; we like this one:\n",
    "- [Linear Regression in Python](https://realpython.com/linear-regression-in-python/)\n",
    "\n",
    "To perform our linear regressions today, we'll be using `scipy.stats` again, but this time a different function: it's now called `linregress`.\n",
    "\n",
    "For both the correlation and linear regression options, the quantities we'll need are:\n",
    "- the coefficient itself (either Pearson's R or regression slope), and\n",
    "- the p-value associated with the correlation or linear fit.\n",
    "\n",
    "These will be calculated not just once, but for **every single lat/lon point in your data**!\n",
    "\n",
    "### **ENSO and the NINO3.4 Region**\n",
    "\n",
    "We'll use a slightly different example in this tutorial: instead of looking just at atmospheric fields, we'll be performing a common analysis that's used in climate research. This is a correlation/regression with the _NINO3.4_ region: this is a pre-defined box in latitude and longitude that's considered to be an _\"index region\"_ for the El Nino/Southern Oscillation, or ENSO. \n",
    "\n",
    "#### **What is ENSO?**\n",
    "\n",
    "If you haven't heard about it before, ENSO is a major _mode_ of climate variability that involves both the atmosphere and the ocean: they talk to each other, or are what we call \"coupled\" in climate research. ENSO has a _period_ that's irregular but is about 5-7 years, and alternates between two _phases_: El Nino and La Nina. During an El Nino event, the equatorial Pacific is _warmer_ than average, and during a La Nina, it's _colder_ than average. \n",
    "\n",
    "Here is a good explainer about what ENSO is from [NOAA's climate.gov blog](https://www.climate.gov/news-features/blogs/enso/what-el-ni%C3%B1o%E2%80%93southern-oscillation-enso-nutshell)!\n",
    "\n",
    "**_Why do we care?_** The pattern of equatorial Pacific temperature is really important! It can cause changes to weather patterns all over the world, drive coral bleaching in the tropical oceans, and cause major economic damages. \n",
    "\n",
    "Getting back to the NINO3.4 region: this is an \"index\" for ENSO because it sits right in the middle of where El Nino and La Nina events have their peak heating or cooling. So if we take an average from a box in the middle, we have a good idea of what type of ENSO conditions we're currently experiencing. \n",
    "\n",
    "Here is what NINO3.4 looks like on a map (image source [NOAA](https://www.climate.gov/media/5541) again):\n",
    "\n",
    "<a href=\"https://www.climate.gov/sites/default/files/Fig3_ENSOindices_SST_large.png\">\n",
    "  <img src=\"https://www.climate.gov/sites/default/files/Fig3_ENSOindices_SST_large.png\" alt=\"image\" width=\"400\">\n",
    "</a>\n",
    "\n",
    "(note: there are also a bunch of OTHER index regions with different numbers, that are used for different aspects of ENSO science. We won't worry about all of them today, but it's good to know that they're out there!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b926a2e-d008-4bf0-834f-e99e513d4f10",
   "metadata": {},
   "source": [
    "<a id='load'></a> \n",
    "## **Load and Query the CMIP6 Catalog**\n",
    "\n",
    "Now we can use `intake` to load the information associated with the CMIP6 holdings on Amazon Web Services! More detail on that database is available on the [Amazon Registry of Open Data](https://registry.opendata.aws/cmip6/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e15e4e-4bfb-438e-be71-f65530c7e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CMIP6 data catalog, store as a variable\n",
    "catalog = intake.open_esm_datastore('https://cmip6-pds.s3.amazonaws.com/pangeo-cmip6.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cca45-d0ff-44d4-81e3-96b7fa5896e8",
   "metadata": {},
   "source": [
    "We'll use the same techniques we learned when we were working on [ensemble analyses](https://github.com/climate-datalab/EnsembleAnalysis) with CMIP6: here let's just stick to a single model to make things simpler for now. (Many research projects do involve looking at correlations and regressions with multiple models though, so that's something that's good to be aware of!)\n",
    "\n",
    "Start with specifying the search terms you want: as a starting example, we can just use the historical information from CESM2. \n",
    "\n",
    "_One thing that's different_ here from the previous work we were doing - although we're grabbing a single set of experiments, we'll want multiple _variables_! In this case, the ones we're interested in are:\n",
    "- tos, the \"temperature at the ocean surface\" (this is how we'll calculate the NINO3.4 index); and\n",
    "- pr, the precipitation field that we'll correlate/regress on NINO3.4.\n",
    "\n",
    "Because tos is an ocean field and pr is an atmosphere field, I have include both \"Amon\" and \"Omon\" in the `table_id` part of the query. You need to include these because otherwise both daily and monthly information for tos and pr will be returned by the search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206a6e9-f953-4c22-96ef-57fc2c844984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify search terms to query catalog \n",
    "# activity_id: which downscaling technique do you want?\n",
    "activity_id = [\"\"]\n",
    "\n",
    "# experiment_id: which historical/future scenario do you want?\n",
    "experiment_id = [\"\"]\n",
    "\n",
    "# table_id: which part of the Earth system/time resolution do you want?\n",
    "table_id = []\n",
    "\n",
    "# source_id: which model do you want?\n",
    "source_id = [\"\"]\n",
    "\n",
    "# variable_id: which variable do you want?\n",
    "variable_id = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359af30-5f9f-4f31-986d-6a2c7331b5e6",
   "metadata": {},
   "source": [
    "Let's extract the catalog information for the above query, then take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1ac73-0910-46bd-9780-7585bef56ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search through catalog, store results\n",
    "cesm_res = catalog.search(activity_id=activity_id, experiment_id=experiment_id,\n",
    "                         source_id=source_id, table_id=table_id, variable_id=variable_id)\n",
    "\n",
    "# Create a data frame\n",
    "\n",
    "# Look at results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e0b084-2779-43f8-a5e1-a852af62d08e",
   "metadata": {},
   "source": [
    "There is a lot of information here! Let's explore it a little bit to make sure we understand what's going on. For instance: there are some entries for the `tos` data that look almost identical...\n",
    "\n",
    "The below code block extracts just the portion of the `cesm_df` data frame containing tos data, then compares two entries to see what's actually different between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86df75a-7716-43a6-8f10-c98a81a54542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look just at the tos entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecde4b0-a64f-4e2a-b1b4-1e7ca0f6be0c",
   "metadata": {},
   "source": [
    "The two entries above differ only in their `grid_label`! (well also in the zstore entry, but that's to be expected since they're two different zarr stores). One grid_label entry is `gr` and one is `gn`: you may recall from the [Opening and Querying the CMIP6 Catalog](https://github.com/climate-datalab/CMIP6_AWS/blob/main/1.%20Opening%20and%20Querying%20the%20CMIP6%20Catalog.ipynb) tutorial that `gn` means \"grid native\", or data that has not been interpolated to a grid other than the one used when running the model. The other entry, `gr`, means \"grid regular\", or data that _has_ been put on a new grid after the model was run.\n",
    "\n",
    "Let's confirm by displaying the data in more detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a8621-72a6-4299-a2f7-e78b71822f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open first CESM tos dataset to look at it\n",
    "test1 = xr.open_zarr(cesm_tos['zstore'][0], storage_options={'anon': True})\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a8351-bb82-4b88-8b2e-eee8cd88bcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open second CESM tos dataset to look at it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9204712-63b5-49aa-9a0f-cc0e086513de",
   "metadata": {},
   "source": [
    "Sure enough, in the first case (`grid_label = 'gr'`), the latitude and longitude are on a _regular grid_ spaced every 1 degree! The second one (`grid_label = 'gn'`) has much more irregularly spaced and numerous lat/lon values: and you may ALSO have noticed that this one is using _two-dimensional_ lat/lon values, whereas in the first example the lat/lon are 1D vectors.\n",
    "\n",
    "The above was just to explore - but now that we have a better idea of what we're looking for, it's clear that the `gr` values are going to be easier to work with for these purposes. So let's stick with those, and redo our search query!\n",
    "\n",
    "NOTE: if you read the code blocks above closely, you may have noticed that the PRECIPITATION values are on the native grid (`grid_label=gn`). So in order to extract just the tos data that's on the REGULAR grid (`grid_label=gr`), we'll have to separate our search queries this time. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02af24-ca68-4711-be9f-22b2624e29ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search through catalog, store results for tos\n",
    "cesm_res_tos = catalog.search(activity_id=activity_id, experiment_id=experiment_id,\n",
    "                         source_id=source_id, table_id=table_id, \n",
    "                         variable_id='tos', grid_label='gr')\n",
    "\n",
    "# Recreate the data frame\n",
    "\n",
    "# Display the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368b76f-98d7-4417-b298-d62b7509535a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search through catalog, store results for precip\n",
    "cesm_res_pr = catalog.search(activity_id=activity_id, experiment_id=experiment_id,\n",
    "                         source_id=source_id, table_id=table_id, \n",
    "                         variable_id='pr')\n",
    "\n",
    "# Recreate the data frame\n",
    "\n",
    "# Display the data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574495c-cddc-495e-9b16-71bf12fbe574",
   "metadata": {},
   "source": [
    "OK now we're in business! There are the same number of entries for `tos` and `pr` now, so let's start getting the data I/O sorted out. \n",
    "\n",
    "Even though the ensemble member entries look identical in this case, let's still go through the exercise of finding the members that the two variables have in common - this will come in handy if you're working with a model where they did NOT provide data for every member for every variable (this happens surprisingly often in CMIP6 analyses). The code steps needed to do this are exactly the same as the ones we used previously, in the [Ensemble Spread and Statistical Significance](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/3.%20Ensemble%20Spread%20and%20Statistical%20Significance.ipynb) tutorial. The only difference is that now we're comparing the common members between two _variables_, not between two different _model experiments_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcde45-f331-4fb4-9b5f-f7d62aed7d94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define list of ensemble members\n",
    "# tos\n",
    "\n",
    "# pr\n",
    "\n",
    "# Find the ones each list has in common\n",
    "\n",
    "# Print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57135d5-4ea5-41d5-a3f6-5488eae68ba7",
   "metadata": {},
   "source": [
    "Now we can use our understanding of subsampling data frames based on the `member_id` field to loop through all of the members for which tos and pr data are available, find their location in the appropriate data frame, and add to an output list. \n",
    "\n",
    "For more details on this, you can refer back to the [Ensemble Spread and Statistical Significance](https://github.com/climate-datalab/EnsembleAnalysis/blob/main/3.%20Ensemble%20Spread%20and%20Statistical%20Significance.ipynb) tutorial for a full explanation.\n",
    "\n",
    "**Brief note:** in the loop below, I have added the `.load()` command to the data extraction step in `xr.open_zarr`, in order to force Python to load the data into memory before appending it to the output list. Not doing this resulted in an error in the _next_ code block (which involves concatenating data using `xr.concat`) related to Python not being able to _rechunk_ the data. Rechunking isn't really the point of this tutorial, but if you're curious about it you can check out the [explainer on parallel computing with Dask](https://docs.xarray.dev/en/stable/user-guide/dask.html) to see what's going on under the hood!\n",
    "\n",
    "On the Bren School \"workbench1\" server, the code block below takes roughly 2-3 minutes to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee8fc6-094a-482d-b8a1-cbb356e78582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define empty lists for output data\n",
    "cesm2_tempdata = []\n",
    "cesm2_prdata = []\n",
    "\n",
    "# Loop over all common ensemble members; grab both temperature\n",
    "# and precip for each member\n",
    "for mem in range(len(common_mems)):\n",
    "    print(common_mems[mem])\n",
    "\n",
    "    # Extract member of interest\n",
    "    prec_cesm2df = cesm_pr_df[(cesm_pr_df[\"member_id\"] == common_mems[mem])]\n",
    "    tos_cesm2df = cesm_tos_df[(cesm_tos_df[\"member_id\"] == common_mems[mem])]\n",
    "    \n",
    "    # Store data as xarray\n",
    "    \n",
    "    # Reassign time for all entries aside from the first one\n",
    "        \n",
    "    # Add to list\n",
    "    cesm2_tempdata.append(hist_tos)\n",
    "    cesm2_prdata.append(hist_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9047ceb-f2fc-4aa7-83e5-c493934d1258",
   "metadata": {},
   "source": [
    "Now we follow our standard steps of concatenating the data into xarray objects - one for each variable - and adding the member names as values in the new \"member\" dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e386f-3191-467e-b4d0-f21ec50efba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the list into a single xarray object\n",
    "cesm2_tempdata = xr.concat(cesm2_tempdata, dim=\"member\")\n",
    "cesm2_prdata = xr.concat(cesm2_prdata, dim=\"member\")\n",
    "\n",
    "# Store the actual member information as values of the new dimension\n",
    "cesm2_tempdata = cesm2_tempdata.assign_coords(member=(\"member\", common_mems))\n",
    "cesm2_prdata = cesm2_prdata.assign_coords(member=(\"member\", common_mems))\n",
    "\n",
    "# Reformat time dimension for concatenated array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5a5fa-1eac-411a-828b-fee636d5c22b",
   "metadata": {},
   "source": [
    "<a id='time_series'></a> \n",
    "## **Calculate Regionally Averaged Time Series**\n",
    "\n",
    "Here we need to step back a little and remember what our goal is. We're aiming to see how temperature in ONE place (the NINO3.4 index region) is related to precipitation in EVERY OTHER place (here, the rest of the world). That means we have one more thing to do before we can calculate the correlation/regression maps: namely, to actually make time series for our _target region_ of NINO3.4. Luckily, we already know how to do this from previous tutorials!\n",
    "\n",
    "The code block below specifies a lat/lon region and performs the regional average from the `cesm2_tempdata` xarray variable we already defined. (note: here I'm not going to bother with area weighting because the region is fairly small and I want to make the code more readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014ce0a-5626-4dd6-a01b-0ba43c5ad124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify rough lat/lon bounds for NINO3.4\n",
    "\n",
    "# Define logical mask: True when lat/lon inside the valid ranges, False elsewhere\n",
    "mask_lat = (cesm2_tempdata.lat >= lat_min) & (cesm2_tempdata.lat <= lat_max)\n",
    "mask_lon = (cesm2_tempdata.lon >= lon_min) & (cesm2_tempdata.lon <= lon_max)\n",
    "\n",
    "# Find points where the mask value is True, drop all other points\n",
    "cesm2_nino34 = cesm2_tempdata.where(mask_lat & mask_lon, drop=True)\n",
    "\n",
    "# Average over the lat and lon dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14fd0e2-7826-4779-9be5-35b41e8fcad4",
   "metadata": {},
   "source": [
    "### **Calculate anomalies**\n",
    "\n",
    "We also want to do one more thing to the time series: that is, to calculate the _anomaly_ by removing the _climatological average_ from the data. We'll do this using the xarray `groupby` command: if you'd like a reminder on how this all works, please refer to the [Anomalies](https://github.com/climate-datalab/Time-Series-Plots/blob/main/4.%20Anomalies.ipynb) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8aa0b-29df-4022-85b9-fb14139a1821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate seasonal average\n",
    "\n",
    "# Print output to see what it looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a557396-a3c8-49a7-bf9e-44c2a7db3af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove seasonal mean to get the anomaly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ead71-684c-4ef9-b4e3-3711e0a5f466",
   "metadata": {},
   "source": [
    "Just as a quick sanity check, let's plot the NINO3.4 anomaly time series for the first ensemble member (`.isel(member = 0)`). This should give you numerical values that range between roughly -4 and +4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aa5d78-8b67-4513-b6f5-2d9053d38464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd122693-b649-4feb-8696-cd94285cffc8",
   "metadata": {},
   "source": [
    "<a id='corr'></a> \n",
    "## **Calculate Correlation or Regression Coefficients**\n",
    "\n",
    "Now the fun part begins - we get to calculate our coefficient values! Let's define a \"flag\", or variable that keeps track of which type of coefficient we'd like to compute: this will let us quickly switch back and forth between regression and correlation if we change our minds about what we'd like to do later on. Here I've arbitrarily defined 0 to mean a correlation coefficient and 1 to mean a regression; you can change this to whatever you want! The only trick is to keep track of it later on when you're setting the relevant if statements (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a8298-6c07-46f3-b97f-1b5577d5f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of coefficient\n",
    "# 0 = correlation, 1 = regression\n",
    "coef_type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d693b4-190f-4032-b1df-edbed1ce4ed6",
   "metadata": {},
   "source": [
    "### **Handling multiple dimensions: `apply_ufunc`**\n",
    "\n",
    "If we were working with data that was just a single time series, the next part would be simple: stick the data into either the `pearsonr` or `linregress` function, look at the results, and be done with it. But climate data is more complicated than that! Instead of one dimension, we have FOUR: time, latitude, longitude, and member. That means that we need a way to make Python loop over those dimensions and deal with them appropriately!\n",
    "\n",
    "Luckily, within xarray there's a function that does exactly what we want: `apply_ufunc`. This applies a function defined by the user, across a _dimension_ within a dataset that's also defined by the user. In this case, what we want to do is:\n",
    "- apply a CORRELATION or REGRESSION function\n",
    "- across the dimension of TIME\n",
    "\n",
    "Technically, this means that we need to define our own function, that essentially says \"Hello, I am a function that applies [correlation/regression] to data you give me\". Here's an example of how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1198d-98b0-4768-bfb8-77a1c524499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correlation and regression functions\n",
    "\n",
    "# 1) correlation\n",
    "# Input data: two time series called x and y\n",
    "\n",
    "    # Get rid of any Nan/Inf values\n",
    "    \n",
    "    # Make sure there is still data\n",
    "    \n",
    "    # Output data: R, p-value for x vs y\n",
    "    \n",
    "\n",
    "# 2) regression\n",
    "# Input data: two time series called x and y\n",
    "\n",
    "    # Get rid of any Nan/Inf values\n",
    "\n",
    "    # Make sure there is still data\n",
    "    \n",
    "    # Perform regression on good data, return slope and p-value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2873963-e4db-4eb6-b525-7b08f53f355b",
   "metadata": {},
   "source": [
    "Now that we have both of our functions defined, we next have to apply them to our input data - then we'll have some nice maps to plot!\n",
    "\n",
    "### **Note on correlation/regression and ensembles**\n",
    "\n",
    "Here it's important to explain one concept that can be tricky when getting started. In our particular case we're only using ONE ensemble member. But you might have data from multiple members... and in that case, you need to be careful. \n",
    "\n",
    "_Rule of thumb: ALWAYS calculate correlations/regressions INDIVIDUALLY for EACH ensemble member!_\n",
    "\n",
    "I've stated the above rule of thumb, because when you're doing a correlation or regression map, you're usually looking for a relationship between two variables **through time**. If you do something like average the ensemble members together _before_ doing the regression or correlation, you'll end up destroying the temporal relationship between your variables! That's because, as we've talked about in other tutorials, each ensemble member has a _different_ time history of climate variability: so on average, they'll end up canceling each other out!\n",
    "\n",
    "_Other rule of thumb: It's totally fine to average correlation/regression maps AFTER you calculate them for each member!_\n",
    "\n",
    "This may sound a little counter-intuitive, but it **is** ok to average your maps together after you make them separately for each member! That's because once you've identified the correlation/regression relationships _within_ ensemble members, they'll \n",
    "\n",
    "a) usually be pretty similar to each other, and\n",
    "b) be _independent estimates_ of the temporal relationship between your two variables (since each ensemble member is independent of the others).\n",
    "\n",
    "So it can actually make your statistics BETTER to average things together... as long as you do it AFTER the correlation/regression calculation!\n",
    "\n",
    "### **Applying the correlation/regression function to data**\n",
    "\n",
    "Let's now go ahead and actually do this thing. We'll need to loop over three of our four dimensions:\n",
    "- ensemble member\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "and for each of those iterations, use `xr.apply_ufunc()` to apply our user-defined functions to the time series at that location/member.\n",
    "\n",
    "#### Aligning time dimension\n",
    "\n",
    "I'm going to add in one more step, since I was getting errors during the course of developing this tutorial. There are VERY SLIGHT (think one second) offsets between the time values in the tos and pr datasets - this led to an error that looked like this when I tried to run the code block below:\n",
    "`ValueError: cannot align objects with join='exact' where index/labels/sizes are not equal along these coordinates (dimensions): 'time' ('time',)`\n",
    "\n",
    "To force this error to disappear, I'm just going to overwrite the `time` coordinate in the precipitation dataset with the one from the tos data - since we happen to know that these SHOULD be the same, this is a fine thing to do. However, _this type of thing needs to be done very carefully_ since you run the risk of using incorrect values if you're not careful! Always triple check before overwriting any data in arrays that you'll be using for your analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf1556-a625-4c4e-b0f9-704af085c5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Force tos, pr time dimensions to match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320ad8a-8f62-4f05-8e09-95a1e92bdef4",
   "metadata": {},
   "source": [
    "#### Setting up output arrays, formatting data\n",
    "\n",
    "Now we need to define output arrays, where the results of the gridpoint correlations/regressions will be stored. This is done in Numpy in the code block below. I've also done a couple other tweaks to the data:\n",
    "- used `.isel()` to select a SINGLE ensemble member (the code was running very slowly when considering all members); and\n",
    "- used the Numpy `broadcast_to()` method to create duplicate copies of the NINO3.4 index time series at all lat and lon values in the output precipitation array. This is called broadcasting since you're \"sending\" the same values to all locations, and will trick the xarray `apply_ufunc()` method into behaving correctly later.\n",
    "\n",
    "If you do want to perform your analysis on all ensemble members, you can just delete the `.isel()` part of the code below - note that this will result in your outputs having an additional dimension, namely \"member\". You'll then need to average across that dimension before plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce80f09-1984-432a-8740-8ffb5b4c80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from a single ensemble member to make things run faster\n",
    "\n",
    "# broadcast NINO3.4 across lat/lon dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce059c-d720-4ff3-83f2-0d575b198896",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we perform the correlation! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ec6ed-d441-470c-86f7-e26a362fa219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use apply_ufunc to do the looping over lat/lon, for a single member\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e716196-5cf3-4685-93b1-b39a1fabd6c1",
   "metadata": {},
   "source": [
    "<a id='maps'></a> \n",
    "## **Make Maps**\n",
    "\n",
    "Now that we've done all the hard parts, it's time to finally plot the results! We'll just use the output from the previous code blocks and lay them onto our map, similar to what was done in previous tutorials in this repo.\n",
    "\n",
    "As a nice bonus, we'll ALSO add code to stipple (plot dots where) the locations where the significance levels are above 90%!\n",
    "\n",
    "The below code block plots results for the correlation coefficient option - if you'd like to use the regression coefficients, just change the appropriate plot labels to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebde24-27e8-46af-a400-96e057d8ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results!\n",
    "map = ccrs.PlateCarree(central_longitude=180)\n",
    "\n",
    "# Create figure/axis objects, use the map object to specify associated projection\n",
    "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw={\"projection\": map})\n",
    "# Plot data onto those axes\n",
    "plot = ax.pcolormesh(this_pr.lon, this_pr.lat, coef_vals.squeeze(), \n",
    "                     transform=ccrs.PlateCarree(), cmap=\"BrBG\", vmin=-1, vmax=1, shading='auto')# \n",
    "# Add colorbar and label it\n",
    "\n",
    "# Add coastline/border lines\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "# Add grid lines\n",
    "gl = ax.gridlines(draw_labels=True, linestyle=\"--\") \n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "# Set font size for x, y-axis labels\n",
    "gl.xlabel_style = {'size': 20}\n",
    "gl.ylabel_style = {'size': 20}\n",
    "\n",
    "# Statistical significance for historical vs future precip change regressed on gradient change\n",
    "\n",
    "# Add title, show plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19073f5b-3f60-44fb-b635-4bf2746b8f75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda 3 (EDS296)",
   "language": "python",
   "name": "eds196-stevenson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
